{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import platform\n",
    "import os\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8fb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_direc = os.getcwd() + \"/\"\n",
    "if platform.system() == \"Windows\":\n",
    "    data_direc.replace(\"/\", \"\\\\\")\n",
    "train = pd.read_csv(data_direc + \"gisette_train.csv\")\n",
    "train_labels = pd.read_csv(data_direc + \"gisette_train_labels.csv\")\n",
    "test = pd.read_csv(data_direc + \"gisette_valid.csv\")\n",
    "test_labels = pd.read_csv(data_direc + \"gisette_valid_labels.csv\")\n",
    "train = np.delete(train, 5000, axis=1)\n",
    "test = np.delete(test, 5000, axis=1)\n",
    "\n",
    "train_labels_array = (train_labels.squeeze() + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32b3598b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "# Define Lorenz loss function\n",
    "def lorenz_loss(beta, train, labels, mu):\n",
    "    z = np.dot(train, beta)\n",
    "    loss = np.sum(np.log(1+np.exp(-labels.T*z))) + 0.5*mu*np.linalg.norm(beta)**2\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fb2dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train = scaler.fit_transform(train)\n",
    "test = scaler.transform(test)\n",
    "\n",
    "def fsa(train, train_labels, test, test_labels, k):\n",
    "    s=.001\n",
    "    mu = 300\n",
    "    max_iter = 300\n",
    "    beta = np.zeros(train.shape[1])\n",
    "    losses = []\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        # define the loss functio wrt beta\n",
    "        loss_function = lambda beta: lorenz_loss(beta, train, train_labels, mu)\n",
    "\n",
    "        # minimize the loss function to update beta\n",
    "        result = minimize(loss_function, beta, method='L-BFGS-B')\n",
    "        beta = result.x\n",
    "\n",
    "        #compute the loss\n",
    "        loss = lorenz_loss(beta, train, train_labels, mu)\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # select top k features\n",
    "    feature_indices = np.argsort(np.abs(beta))[-k:]\n",
    "\n",
    "    # train logistic regression with selected features\n",
    "    lr = LogisticRegression(max_iter)\n",
    "    lr.fit(train[:, feature_indices], train_labels)\n",
    "    \n",
    "    # compute misclassification error\n",
    "    train_error = 1 - accuracy_score(train_labels, lr.predict(train[:, feature_indices]))\n",
    "    test_error = 1 - accuracy_score(test_labels, lr.predict(test[:, feature_indices]))\n",
    "\n",
    "    return train_error, test_error, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51807d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/data_analysis/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The user-provided objective function must return a scalar value.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/opt/miniconda3/envs/data_analysis/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:141\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     fx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m loss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m beta: lorenz_loss(beta, train, train_labels, mu)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# minimize the loss function to update beta\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_analysis/lib/python3.10/site-packages/scipy/optimize/_minimize.py:710\u001b[0m, in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    707\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m    708\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 710\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    713\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[1;32m    714\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_analysis/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:307\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[0;32m--> 307\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[1;32m    313\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_analysis/lib/python3.10/site-packages/scipy/optimize/_optimize.py:383\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    379\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_analysis/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(grad):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_analysis/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_analysis/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/data_analysis/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:143\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    141\u001b[0m         fx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(fx)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 143\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    144\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe user-provided objective function \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust return a scalar value.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fx \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_f:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lowest_x \u001b[38;5;241m=\u001b[39m x\n",
      "\u001b[0;31mValueError\u001b[0m: The user-provided objective function must return a scalar value."
     ]
    }
   ],
   "source": [
    "beta = np.zeros(train.shape[1])\n",
    "mu = 300\n",
    "\n",
    "# define the loss function wrt beta\n",
    "loss_function = lambda beta: lorenz_loss(beta, train, train_labels, mu)\n",
    "\n",
    "# minimize the loss function to update beta\n",
    "result = minimize(loss_function, beta, method='L-BFGS-B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71685ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/data_analysis/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: The behavior of DataFrame.sum with axis=None is deprecated, in a future version this will reduce over both axes and return a scalar. To retain the old behavior, pass axis=0 (or do not pass axis)\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0.693147\n",
       "1       0.693147\n",
       "2       0.693147\n",
       "3       0.693147\n",
       "4       0.693147\n",
       "          ...   \n",
       "5995    0.693147\n",
       "5996    0.693147\n",
       "5997    0.693147\n",
       "5998    0.693147\n",
       "5999    0.693147\n",
       "Length: 6000, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.dot(train, beta)\n",
    "loss = np.sum(np.log(1+np.exp(-train_labels.T*z))) + 0.5*mu*np.linalg.norm(beta)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dc9ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use different k values\n",
    "k_values = [10, 30, 100, 300, 500]\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "train_losses = []\n",
    "\n",
    "# train the FSA classifier for different k values\n",
    "for k in k_values:\n",
    "    train_error, test_error, losses = fsa(train, train_labels, test, test_labels, k)\n",
    "    train_errors.append(train_error)\n",
    "    test_errors.append(test_errors)\n",
    "    train_losses.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4961f686",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920fd436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training loss vs iteration number for k=100\n",
    "plt.plot(range(300), train_losses[2]) #index 2 for k=100\n",
    "plt.title(\"Training Loss vs Iteration Number for k = 100\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5933651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b452c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# report misclassification errors in a table\n",
    "print(\"k\\tTrain Error\\tTest Error\")\n",
    "for i, k in enumerate(k_values):\n",
    "    print(f\"{k}\\t{train_errors[i]:.4f}\\t\\t{test_errors[i]:.4f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e04054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot misclassification error vs k\n",
    "plt.plot(k_values, train_errors, label = \"Train Error\")\n",
    "plt.plot(k_values, test_errors, label = \"Test Error\")\n",
    "plt.title(\"Misclassification Error vs Number of Features\")\n",
    "plt.xlabel(\"Number of Features (k)\")\n",
    "plt.ylabel(\"Misclassification Error\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef67a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the ROC curves for k=100\n",
    "k_100_features = np.argsort(np.abs(beta))[-100:]\n",
    "lr_100_features = LogisticRegression(max_iter)\n",
    "lr_100_features.fit(train[:, k_100_features], train_labels)\n",
    "train_labels_pred = lr_100_features.predict_proba(train[:, k_100_features])[:,1]\n",
    "test_labels_pred = lr_100_features.predict_proba(test[:, k_100_features])[:,1]\n",
    "\n",
    "train_false_pos_rate, train_true_pos_rate, _ = roc_curve(train_labels, train_labels_pred)\n",
    "test_false_pos_rate, test_true_pos_rate, _ = roc_curve(test_labels, test_labels_pred)\n",
    "\n",
    "roc_auc_train = auc(train_false_pos_rate, train_true_pos_rate)\n",
    "roc_auc_test = auc(test_false_pos_rate, test_true_pos_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ba8c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(train_false_pos_rate, train_true_pos_rate,\n",
    "    label=\"Training ROC Curve (AUC = {:.2f})\".format(roc_auc_train))\n",
    "plt.plot(false_pos_test, true_pos_test, label=\"Test ROC Curve (AUC = {:.2f})\".format(roc_auc_test))\n",
    "plt.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"Receiver Operating Characteristic (ROC) Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670f5e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc51ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "data_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
