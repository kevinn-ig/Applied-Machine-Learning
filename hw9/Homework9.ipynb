{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0183dd",
   "metadata": {},
   "source": [
    "# Homework \n",
    "Evelina Teran & Kevin Smith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "435a0250-01ee-4e54-8b12-2979698eb6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from IPython.display import display_markdown\n",
    "import platform\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4667af",
   "metadata": {},
   "source": [
    "## Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4dd9b39-a4bb-4535-a1f2-ad6a6b1e26b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.getcwd() + \"/\"\n",
    "if platform.system() == \"Windows\":\n",
    "    file_path = file_path.replace(\"/\", \"\\\\\")\n",
    "    \n",
    "pb1 = pd.read_csv(file_path + \"hmm_pb1.csv\", header=None)\n",
    "pb1 = np.array(pb1).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b6cd83-97e7-4ca7-a3fb-51bcdcf96f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_probs = np.array([[0.95, 0.05], [0.10, 0.90]]) # a values\n",
    "emission_probs = np.array([[1/6, 1/6, 1/6, 1/6, 1/6, 1/6], [1/10, 1/10, 1/10, 1/10, 1/10, 1/2]]) # b values\n",
    "initial_probs = np.array([0.5, 0.5]) # pi values\n",
    "\n",
    "# # Observed sequence (dice rolls)\n",
    "# obs_seq = [1, 3, 5, 2, 4, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc5d8d4-6e6e-40e7-ab1e-9f3348ab21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of hidden states (fair, loaded)\n",
    "NUM_STATES = 2\n",
    "\n",
    "# Define the number of time steps (observations) on the shape of pb1\n",
    "num_time_steps = pb1.shape[0]\n",
    "\n",
    "# Initialize arrays for storing values\n",
    "log_vit_probs = np.zeros((NUM_STATES, num_time_steps))\n",
    "maximizing_states = np.zeros((NUM_STATES, num_time_steps))\n",
    "most_likely_seq = -1 * np.ones_like(pb1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b40d28c-bc42-4845-a308-4afbc2d57efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the Viterbi algorithm\n",
    "# Calculate intial log probs for each state\n",
    "initial_log_probs = np.log(emission_probs[:, pb1[0] - 1] * initial_probs)\n",
    "log_vit_probs[:, 0] = initial_log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf576310-9316-43e8-8711-f82fab170744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The most probable sequence of the hidden state is:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Iterative step of the Viterbi algorithm\n",
    "for t in range(1, pb1.shape[0]):\n",
    "    # Calculate log probs of transitioning form previous states to current states\n",
    "    transition_log_probs = np.log(transition_probs) + log_vit_probs[:, t-1].reshape(-1,1)\n",
    "\n",
    "    # Calculate log probs of emitting the current observations from each state\n",
    "    emission_log_probs = np.log(emission_probs[:, pb1[t] - 1])\n",
    "\n",
    "    # Compute total log probs for each state at current time step t\n",
    "    total_log_probs = emission_log_probs + np.max(transition_log_probs, axis=0)\n",
    "\n",
    "    # Update Viterbi matrix with total log probs\n",
    "    log_vit_probs[:, t] = total_log_probs\n",
    "\n",
    "    # Store maximizing state for this t\n",
    "    maximizing_states[:, t] = np.argmax(transition_log_probs, axis=0)\n",
    "\n",
    "# Find most likely sequence of hidden states\n",
    "# Initialize last stae in most liekly sequence\n",
    "most_likely_seq[-1] = np.argmax(log_vit_probs[:, -1])\n",
    "\n",
    "# Iterate backward through time steps to determine the most likely sequence\n",
    "for t in range(pb1.shape[0] - 1, 0, -1):\n",
    "    # Determine the state at the current time step based on the pointer from the next time step\n",
    "    most_likely_seq[t - 1] = maximizing_states[most_likely_seq[t], t]\n",
    "\n",
    "# Display the most likely sequence of hidden states\n",
    "display_markdown(\"The most probable sequence of the hidden state is:\", raw=True)\n",
    "print(most_likely_seq + 1)  # Add 1 to each state index for human-readable output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa90649-95a6-40d6-a582-ae28ee1cb43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that implements the forward-backward algorithms\n",
    "def forward_backward(dataset, initial_probs, transition_probs, emission_probs):\n",
    "    log_alpha = np.zeros((2, dataset.shape[0]))\n",
    "    log_beta = np.zeros((2, dataset.shape[0]))\n",
    "\n",
    "    # Compute forward probs\n",
    "    log_alpha[:, 0] = np.log(emission_probs[:, dataset[0] - 1] * initial_probs)\n",
    "    log_alpha[:, 0] -= log_alpha[:, 0].sum()\n",
    "\n",
    "    for t in range(1, dataset.shape[0]):\n",
    "        alpha_numerator = np.log(emission_probs[:, dataset[t] - 1]) + np.log(np.sum(transition_probs * np.exp(log_alpha[:, t - 1].reshape(-1, 1)), axis=0))\n",
    "        alpha_denominator = np.log(np.sum(np.exp(alpha_numerator)))\n",
    "        log_alpha[:, t] = alpha_numerator - alpha_denominator\n",
    "\n",
    "    # Compute backward probs\n",
    "    log_beta[:, -1] = 0.5\n",
    "\n",
    "    for t in range(dataset.shape[0] - 2, -1, -1):\n",
    "        beta_numerator = np.log(np.sum(transition_probs * (np.exp(log_beta[:, t + 1]) * emission_probs[:, dataset[t + 1] - 1]), axis=1))\n",
    "        beta_denominator = np.log(np.sum(np.exp(beta_numerator)))\n",
    "        log_beta[:, t] = beta_numerator - beta_denominator\n",
    "\n",
    "    return log_alpha, log_beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68aaa8e5-7daf-49d6-8f9e-225549565d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The following shows $\\alpha_{138}^1/\\alpha_{138}^2$."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.487100265980972\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The following shows $\\beta_{138}^1/ \\beta_{138}^2$."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.540995425670584\n"
     ]
    }
   ],
   "source": [
    "# Display the result\n",
    "log_alpha, log_beta = forward_backward(pb1, initial_probs, transition_probs, emission_probs)\n",
    "display_markdown(\"The following shows $\\\\alpha_{138}^1/\\\\alpha_{138}^2$.\", raw=True)\n",
    "print(np.exp(log_alpha[0][138] - log_alpha[1][138]))\n",
    "display_markdown(\"The following shows $\\\\beta_{138}^1/ \\\\beta_{138}^2$.\", raw=True)\n",
    "print(np.exp(log_beta[0][138] - log_beta[1][138]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf0ee11-ef41-479b-9d14-a75d5e56aab6",
   "metadata": {},
   "source": [
    "## Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1ebea81-7735-472d-ad56-d46db1181cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pb2 = pd.read_csv(file_path + \"hmm_pb2.csv\", header=None)\n",
    "pb2 = np.array(pb1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5b131b-75cb-4928-b4c4-b211fc88b9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that implements the forward-backward algorithms\n",
    "def forward_backward(dataset, initial_probs, transition_probs, emission_probs):\n",
    "    log_alpha = np.zeros((2, dataset.shape[0]))\n",
    "    log_beta = np.zeros((2, dataset.shape[0]))\n",
    "\n",
    "    epsilon = 1e-10  # Small epsilon value to handle zero probabilities and avoid division by zero\n",
    "\n",
    "    # Compute forward probs\n",
    "    emission_probs_safe = np.maximum(emission_probs[:, dataset[0] - 1], epsilon)\n",
    "    log_alpha[:, 0] = np.log(emission_probs_safe * initial_probs)\n",
    "    log_alpha[:, 0] -= log_alpha[:, 0].sum()\n",
    "\n",
    "    for t in range(1, dataset.shape[0]):\n",
    "        # Handle zero or negative emission probabilities by adding a small epsilon value\n",
    "        emission_probs_safe = np.maximum(emission_probs[:, dataset[t] - 1], epsilon)\n",
    "        \n",
    "        alpha_numerator = np.log(emission_probs_safe) + np.log(np.sum(transition_probs * np.exp(log_alpha[:, t - 1].reshape(-1, 1)), axis=0))\n",
    "        alpha_denominator = np.log(np.sum(np.exp(alpha_numerator)))\n",
    "        log_alpha[:, t] = alpha_numerator - alpha_denominator\n",
    "\n",
    "\n",
    "    # Compute backward probs\n",
    "    log_beta[:, -1] = 0.5\n",
    "\n",
    "    for t in range(dataset.shape[0] - 2, -1, -1):\n",
    "        # Handle zero or negative emission and transition probabilities by adding a small epsilon value\n",
    "        emission_probs_safe = np.maximum(emission_probs[:, dataset[t + 1] - 1], epsilon)\n",
    "        transition_probs_safe = np.maximum(transition_probs, epsilon)\n",
    "        \n",
    "        beta_numerator = np.log(np.sum(transition_probs_safe * (np.exp(log_beta[:, t + 1]) * emission_probs_safe), axis=1))\n",
    "        beta_denominator = np.log(np.sum(np.exp(beta_numerator)))\n",
    "        log_beta[:, t] = beta_numerator - beta_denominator\n",
    "\n",
    "    return log_alpha, log_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "329557f5-ef61-4c73-8047-100676f686fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baum_welch_algorithm(dataset, initial_probs, transition_probs, emission_probs, num_iter=10):\n",
    "    # Initialization\n",
    "    learned_initial_probs = initial_probs # learned pi\n",
    "    learned_transition_probs = transition_probs # learned a\n",
    "    learned_emission_probs = emission_probs # learned b\n",
    "    epsilon = 1e-10\n",
    "\n",
    "    # Create one hot encoded dataset for calculation\n",
    "    encoder = OneHotEncoder(categories = [[1,2,3,4,5,6]], sparse_output=False)\n",
    "    encoded_dataset = np.transpose(encoder.fit_transform(dataset.reshape(-1,1)))\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        # Compute forward and backward probabilities\n",
    "        alpha, beta = forward_backward(dataset, learned_initial_probs, learned_transition_probs, learned_emission_probs)\n",
    "\n",
    "        ## E-step\n",
    "        # Calculate b_x(t+1)^j\n",
    "        next_emission_probs = learned_emission_probs[:, np.roll(dataset - 1, -1)]\n",
    "\n",
    "        # Handle zero or negative emission probabilities by adding a small epsilon value\n",
    "        emission_probs_safe = np.maximum(next_emission_probs, epsilon)\n",
    "        \n",
    "        # Compute xi\n",
    "        xi_numerator = alpha.reshape(2, 1, -1) * learned_transition_probs.reshape(2, 2, 1) * np.roll(\n",
    "            beta.reshape(1, 2, -1), shift=-1, axis=-1) * emission_probs_safe.reshape(1, 2, -1)\n",
    "        xi_denominator = np.sum(xi_numerator, axis=(0, 1)).reshape(1, 1, -1)\n",
    "        xi_denominator[xi_denominator == 0] = epsilon # Ensure we do not divide by zero\n",
    "        xi = xi_numerator / xi_denominator\n",
    "\n",
    "        # Check for division by zero in xi_denominator\n",
    "        assert xi_denominator.min() != 0\n",
    "        \n",
    "        # Calculate gamma (expected number of visits to each state)\n",
    "        gamma_numerator = alpha * beta\n",
    "        gamma_denominator = np.sum(gamma_numerator, axis=0).reshape(1, -1)\n",
    "        gamma_denominator[gamma_denominator == 0] = epsilon  # ensure we don't divide by zero\n",
    "        gamma = gamma_numerator / gamma_denominator\n",
    "        \n",
    "        # Check for division by zero in gamma_denominator\n",
    "        assert gamma_denominator.min() != 0\n",
    "        \n",
    "        ## M-step\n",
    "        # Update parameters based on the expected counts\n",
    "        learned_initial_probs = gamma[:, 0]\n",
    "        learned_transition_probs = np.sum(xi[:, :, :-1], axis=2) / np.sum(gamma[:, :-1], axis=1).reshape(-1, 1)\n",
    "        learned_emission_probs = np.sum(gamma.reshape(2, 1, -1) * encoded_dataset.reshape(1, 6, -1), axis=2) / np.sum(gamma, axis=1).reshape(-1, 1)\n",
    "\n",
    "    return learned_initial_probs, learned_transition_probs, learned_emission_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d111eb8-f4b1-45f7-b0f9-85c91c17f89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Baum-Welch algorithm\n",
    "learned_initial_probs, learned_transition_probs, learned_emission_probs = baum_welch_algorithm(pb2, initial_probs, transition_probs, emission_probs, 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efee53cb-c2ae-4bf9-aa71-ca5c3c15daa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The learned initial probability is:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.28806428 0.71193572]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The learned transition probability is:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46355313 0.53577229]\n",
      " [0.59087941 0.40972353]]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The learned emission probability is:"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.40936207e-01 8.08309542e-12 8.08184746e-12 6.05410064e-12\n",
      "  7.51223751e-12 7.59063793e-01]\n",
      " [3.37287677e-12 2.26580671e-01 2.26580671e-01 2.76931932e-01\n",
      "  2.64344117e-01 5.56260842e-03]]\n"
     ]
    }
   ],
   "source": [
    "# Display the learned parameters\n",
    "display_markdown(\"The learned initial probability is:\", raw=True)\n",
    "print(learned_initial_probs)\n",
    "display_markdown(\"The learned transition probability is:\", raw=True)\n",
    "print(learned_transition_probs)\n",
    "display_markdown(\"The learned emission probability is:\", raw=True)\n",
    "print(learned_emission_probs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
